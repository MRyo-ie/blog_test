---
title: "BERTを最小構成で動かしてみた 1　〜 日本語BERT でMask予測 〜" # Title of the blog post.
date: 2021-04-16T11:10:57+09:00 # Date of post creation.
description: "Article description." # Description used for search engine.
featured: true # Sets if post is a featured post, making it appear on the sidebar. A featured post won't be listed on the sidebar if it's the current page
# draft: true
toc: true # Controls if a table of contents should be generated for first-level links automatically.
# menu: main
# featureImage: "/images/path/file.jpg" # Sets featured image on blog post.
# thumbnail: "/images/path/thumbnail.png" # Sets thumbnail image appearing inside card on homepage.
# shareImage: "/images/path/share.png" # Designate a separate image for social media sharing.
codeMaxLines: 15 # Override global value for how many lines within a code block before auto-collapsing.
codeLineNumbers: true # Override global value for showing of line numbers within code block.
figurePositionShow: true # Override global value for showing the figure label.
categories:
  - AI
  - NLP
tags:
  - BERT
  - 動かしてみた
---


## はじめに
今更ですが、BERT（[huggingface/transformers](https://github.com/huggingface/transformers)）を動かしてみたので、その備忘録です。  
全5回シリーズを投稿する予定で、本稿はその第１回目になります。

なお、シリーズは以下のようになる予定です。
1. BERTを最小構成で動かしてみた 1　〜 日本語BERT でMask予測 〜  ← 今ここ
2. BERTを最小構成で動かしてみた 2　〜 日本語BERT で文章分類 fine-tuning 〜
3. BERTをカスタマイズしてみた 1　〜 日本語BERT,T5 でマルチタスクfine-tuning 〜
4. BERTをカスタマイズしてみた 2　〜 huggingface/transformers を色々試して見る 〜

## 環境
- Googloe Colab
- （おまけ）jetson AGX Xavier（失敗？）


## やったこと
東北大、京大 のモデルで、Mask 予測を動かしてみました。

以下、最小コードです。
```
import torch
from transformers import AutoTokenizer, AutoModelForMaskedLM

# トークナイザーとモデルの準備
tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
model = AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')

def predict(input_ids):
    # テキストをテンソルに変換
    input_ids = tokenizer.encode(text, return_tensors='pt')
    masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]
    
    # 推論
    result = model(input_ids)
    pred_ids = result[0][:, masked_index].topk(5).indices.tolist()[0]
    for pred_id in pred_ids:
        output_ids = input_ids.tolist()[0]
        output_ids[masked_index] = pred_id
        print('[Info] pred_id : ', pred_id,  ',\n    ', output_ids)
        print('  ', tokenizer.decode(output_ids))

# テキスト
text = f'吾輩は{tokenizer.mask_token}である。名前はまだない。'
predict(text)
```

[huggingface/transformers](https://github.com/huggingface/transformers) が色々ラップしてくれているので、これだけでOK。すごい。

以下、細かいコードの説明です。

### Tokenizer, Model の読み込み。
huggingface/transformers では、docker-hub のように、事前学習済みモデルを公開できる場所があり、そこに日本語学習済みモデルもいくつか上がっています。（調査中）

その中でも有名なものとして、
- 東北大学のモデル
  - cl-tohoku/bert-base-japanese-v2
  - （旧ver）cl-tohoku/bert-base-japanese-whole-word-masking
- 京都大学のモデル
  - Japanese_L-12_H-768_A-12_E-30_BPE/bert_config.json

があるので、今回は、それらを実際に試してみました。

```
# トークナイザーとモデルの準備
tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
model = AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
```


### Tokenizer, Model の読み込み。
```
def predict(input_ids):
    # テキストをテンソルに変換
    input_ids = tokenizer.encode(text, return_tensors='pt')
    masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]
    
    # 推論
    result = model(input_ids)
    pred_ids = result[0][:, masked_index].topk(5).indices.tolist()[0]
    for pred_id in pred_ids:
        output_ids = input_ids.tolist()[0]
        output_ids[masked_index] = pred_id
        print('[Info] pred_id : ', pred_id,  ',\n    ', output_ids)
        print('  ', tokenizer.decode(output_ids))
```


### 結果
```
text = f'吾輩は{tokenizer.mask_token}である。名前はまだない。'
predict(text)
```

実行結果は、↓ のようになりました。

```
[Info] pred_id :  6040 ,
     [2, 7184, 30046, 9, 6040, 12, 31, 8, 1381, 9, 2941, 80, 8, 3]
   [CLS] 吾輩 は 猫 で ある 。 名前 は まだ ない 。 [SEP]
[Info] pred_id :  2928 ,
     [2, 7184, 30046, 9, 2928, 12, 31, 8, 1381, 9, 2941, 80, 8, 3]
   [CLS] 吾輩 は 犬 で ある 。 名前 は まだ ない 。 [SEP]
[Info] pred_id :  1410 ,
     [2, 7184, 30046, 9, 1410, 12, 31, 8, 1381, 9, 2941, 80, 8, 3]
   [CLS] 吾輩 は 人間 で ある 。 名前 は まだ ない 。 [SEP]
[Info] pred_id :  10082 ,
     [2, 7184, 30046, 9, 10082, 12, 31, 8, 1381, 9, 2941, 80, 8, 3]
   [CLS] 吾輩 は 狼 で ある 。 名前 は まだ ない 。 [SEP]
[Info] pred_id :  1325 ,
     [2, 7184, 30046, 9, 1325, 12, 31, 8, 1381, 9, 2941, 80, 8, 3]
   [CLS] 吾輩 は 私 で ある 。 名前 は まだ ない 。 [SEP]
```

```
text = f'テレビで{tokenizer.mask_token}の試合を見る。'
predict(text)
```
```
[Info] pred_id :  1301 ,
     [2, 571, 12, 1301, 5, 608, 11, 2867, 8, 3]
   [CLS] テレビ で サッカー の 試合 を 見る 。 [SEP]
[Info] pred_id :  5653 ,
     [2, 571, 12, 5653, 5, 608, 11, 2867, 8, 3]
   [CLS] テレビ で バスケットボール の 試合 を 見る 。 [SEP]
[Info] pred_id :  15235 ,
     [2, 571, 12, 15235, 5, 608, 11, 2867, 8, 3]
   [CLS] テレビ で 阪神タイガース の 試合 を 見る 。 [SEP]
[Info] pred_id :  542 ,
     [2, 571, 12, 542, 5, 608, 11, 2867, 8, 3]
   [CLS] テレビ で 代表 の 試合 を 見る 。 [SEP]
[Info] pred_id :  91 ,
     [2, 571, 12, 91, 5, 608, 11, 2867, 8, 3]
   [CLS] テレビ で 日本 の 試合 を 見る 。 [SEP]
```

あっさり出来てしまって、結構びっくりしました。



## おまけ
### Jetson AGX Xavier で動かしてみた
予測結果
```
[CLS] 吾輩 は し で ある 。 名前 は まだ ない 。 [SEP]
[CLS] 吾輩 は 一 で ある 。 名前 は まだ ない 。 [SEP]
[CLS] 吾輩 は と で ある 。 名前 は まだ ない 。 [SEP]
[CLS] 吾輩 は 数 で ある 。 名前 は まだ ない 。 [SEP]
[CLS] 吾輩 は 生まれ で ある 。 名前 は まだ ない 。 [SEP]
```

なぜかうまくいかなかったです...。

一応、デバッグのために id を表示してみたけど...
```
[Info] pred_id :  15 ,
     [2, 7184, 30046, 9, 15, 12, 31, 8, 1381, 9, 2941, 80, 8, 3]
   [CLS] 吾輩 は し で ある 。 名前 は まだ ない 。 [SEP]
[Info] pred_id :  52 ,
     [2, 7184, 30046, 9, 52, 12, 31, 8, 1381, 9, 2941, 80, 8, 3]
   [CLS] 吾輩 は 一 で ある 。 名前 は まだ ない 。 [SEP]
[Info] pred_id :  13 ,
    ・・・
[Info] pred_id :  276 ,
    ・・・
[Info] pred_id :  1115 ,
    ・・・
```
```
[Info] pred_id :  15 ,
     [2, 571, 12, 15, 5, 608, 11, 2867, 8, 3]
   [CLS] テレビ で し の 試合 を 見る 。 [SEP]
[Info] pred_id :  52 ,
     [2, 571, 12, 52, 5, 608, 11, 2867, 8, 3]
   [CLS] テレビ で 一 の 試合 を 見る 。 [SEP]
[Info] pred_id :  13 ,
    ・・・
[Info] pred_id :  1115 ,
    ・・・
[Info] pred_id :  281 ,
    ・・・
```

Google Colab のときと id が違うので、Tokenizer 側ではなく、
Model 側に問題があるだろうとの結論に至りました。
が、結局原因は分からず...。

（これからカスタマイズしていくうちに、原因が分かると良いな...）


